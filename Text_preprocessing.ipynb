{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRSRm1FUIkOq32LzlxCVcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joywang233/TwitterPreprocessing/blob/main/Text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "from gensim.utils import deaccent\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu4a2_1S3ZSP",
        "outputId": "c5030df1-bad3-4154-d644-2456e3147980"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  #remove text in square brackets\n",
        "  text = re.sub(r'\\[.*?\\]', '', text)\n",
        "  #remove url\n",
        "  text = re.sub(r\"http\\S+\", \"\", text)\n",
        "  text = re.sub(r\"https\\S+\", \"\", text)\n",
        "  #remove mentioned user\n",
        "  text = re.sub('@[^\\s]+','',text)\n",
        "  #remove punctuation\n",
        "  text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "  #remove rt\n",
        "  text = text.replace('RT', '').replace('\\n', ' ').strip()\n",
        "  text = text.replace('rt', '').replace('\\n', ' ').strip()\n",
        "  text = text.replace('gt', '').replace('\\n', ' ').strip()\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "# Remove emojis\n",
        "def remove_emojis(text):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', text)\n",
        "\n",
        "\n",
        "\n",
        "class WhiteSpacePreprocessing():\n",
        "    \"\"\"\n",
        "    Ref: https://github.com/MilaNLProc/contextualized-topic-models/blob/master/contextualized_topic_models/utils/preprocessing.py\n",
        "    Provides a very simple preprocessing script that filters infrequent tokens from text\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, documents, stopwords_language=\"english\", vocabulary_size=2000):\n",
        "        \"\"\"\n",
        "        :param documents: list of strings\n",
        "        :param stopwords_language: string of the language of the stopwords (see nltk stopwords)\n",
        "        :param vocabulary_size: the number of most frequent words to include in the documents. Infrequent words will be discarded from the list of preprocessed documents\n",
        "        \"\"\"\n",
        "        self.documents = documents\n",
        "        self.stopwords = set(stop_words.words(stopwords_language)) #you may include your customized stopwords list\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "\n",
        "        warnings.simplefilter('always', DeprecationWarning)\n",
        "        warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n",
        "                      \"Use WhiteSpacePreprocessingStopwords.\")\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"\n",
        "        Note that if after filtering some documents do not contain words we remove them. That is why we return also the\n",
        "        list of unpreprocessed documents.\n",
        "        :return: preprocessed documents, unpreprocessed documents and the vocabulary list\n",
        "        \"\"\"\n",
        "        preprocessed_docs_tmp = self.documents\n",
        "        preprocessed_docs_tmp = [deaccent(doc.lower()) for doc in preprocessed_docs_tmp]\n",
        "        preprocessed_docs_tmp = [doc.translate(\n",
        "            str.maketrans(string.punctuation, ' ' * len(string.punctuation))) for doc in preprocessed_docs_tmp]\n",
        "        print('check stop words:', self.stopwords)\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if len(w) > 0 and w not in self.stopwords])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        vectorizer = CountVectorizer(max_features=self.vocabulary_size)\n",
        "        vectorizer.fit_transform(preprocessed_docs_tmp)\n",
        "        temp_vocabulary = set(vectorizer.get_feature_names_out())\n",
        "        #print(len(temp_vocabulary))\n",
        "\n",
        "        preprocessed_docs_tmp = [' '.join([w for w in doc.split() if w in temp_vocabulary])\n",
        "                                 for doc in preprocessed_docs_tmp]\n",
        "\n",
        "        # the size of the preprocessed or unpreprocessed_docs might be less than given docs\n",
        "        # for that reason, we need to return retained indices to change the shape of given custom embeddings.\n",
        "        preprocessed_docs, unpreprocessed_docs, retained_indices = [], [], []\n",
        "        for i, doc in enumerate(preprocessed_docs_tmp):\n",
        "            if len(doc) > 0:\n",
        "                preprocessed_docs.append(doc)\n",
        "                unpreprocessed_docs.append(self.documents[i])\n",
        "                retained_indices.append(i)\n",
        "\n",
        "        vocabulary = list(set([item for doc in preprocessed_docs for item in doc.split()]))\n",
        "        return preprocessed_docs, unpreprocessed_docs, vocabulary, retained_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hyk1Kbj2d6L",
        "outputId": "05f1fc72-ee14-4534-d5e4-b7aefdae73e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<>:8: DeprecationWarning: invalid escape sequence '\\s'\n",
            "<>:8: DeprecationWarning: invalid escape sequence '\\s'\n",
            "<ipython-input-14-92626ad99c5f>:8: DeprecationWarning: invalid escape sequence '\\s'\n",
            "  text = re.sub('@[^\\s]+','',text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read your text file(can be csv or any other format) here\n",
        "raw_text = [\n",
        "    'Interested in a career in #cybersecurity? üíª\\n\\nJoin us on Wednesday, October 19 from 12:00 pm ‚Äì 2:00 pm ET for a career panel with three esteemed leaders in the industry.\\n\\nSpots are limited so reserve yours today ‚¨áÔ∏è\\nhttps://t.co/UTpyoj4Mpr https://t.co/G3ocJNAuU0'\n",
        "    '‚û°Ô∏è‚û°Ô∏èCheck out the 5 main benefits of #Web3!\\n\\n#data #security #cybersecurity #bigdata #privacy #python #javascript #cloud #technology #devcommunity #coding #developers #software #aws #serverless #webdevelopment #opensource #iot https://t.co/UCtna6eme1'\n",
        "    'RT @LetsDefendIO: Cybersecurity Wheel https://t.co/4qDF118WRf'\n",
        "    'Grand raffle prize at @laasersladybugs Bugs, Bags &amp; Brews event!! üêû \\n\\nWin the bags &amp; boards on Saturday at Schram Brewery in Chaska!! üç∫ \\n\\nPlus, become an ally in the fight to #EndtheStigma. All proceeds go to mental health initiatives in public schools.\\n\\n1-6 pm. See you there! https://t.co/vgO4cRco11'\n",
        "    '@AltCryptoGems @beyondprotocol1 has secured a $12.5M in this bear season to roll out it mainnet and building the next big thing: smart contracts for IoT. \\n#DataSecurity #Infosec #100DaysOfCode #Hacking #Cybersecurity #AI #Crypto #CryptoNews #smartcities #smarthome https://t.co/xDkLKyFpsB'\n",
        "    'RT @NandanLohitaksh: IDOR Checklist by @hunter0x7 \\n\\n#bugbounty #bugbountytips #cybersecurity https://t.co/MInHMRrCQL'\n",
        "    ]\n",
        "\n",
        "cleaned_text = [clean_text(txt) for txt in raw_text]\n",
        "cleaned_text = [remove_emojis(txt) for txt in cleaned_text]\n",
        "processor = WhiteSpacePreprocessing(cleaned_text)\n",
        "preprocessed_docs, unpreprocessed_docs, vocab, retained_indices = processor.preprocess()\n",
        "preprocessed_docs #here is the cleaned text in a list of string\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLX_sGMUzgXW",
        "outputId": "59400bf2-061f-484d-fff8-9029ddbc4bb4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check stop words: {'just', 'before', 'wouldn', 'into', 'does', 'between', 'where', \"shouldn't\", 'most', 'having', \"weren't\", 'below', 'we', 's', 'if', 't', 'was', 'had', 'that', \"hasn't\", 'here', 'my', 'now', 'him', 'and', 'how', 'be', 'mustn', 'yourselves', 'doesn', \"needn't\", 'off', 'more', 'the', 'each', \"that'll\", 'they', 'aren', \"won't\", 'will', 'any', 'its', 'theirs', \"mustn't\", 'so', 'shouldn', \"don't\", 'your', 'further', 'herself', 'myself', 'when', 'himself', \"you'll\", 'a', \"didn't\", 'at', \"it's\", 'all', \"wasn't\", 'other', 'wasn', 'd', 'no', 'o', \"doesn't\", 'don', 'of', 'did', 'them', \"hadn't\", 'is', 'been', 'hadn', \"you're\", \"shan't\", 'again', 'can', 'during', 'has', 'yourself', 'about', 'very', \"you'd\", 'above', 'it', 'nor', 'ain', 'ours', 'with', 're', 'she', 'i', 'have', 'from', 've', 'out', 'you', 'in', 'me', \"haven't\", 'her', 'doing', 'm', \"you've\", 'under', 'those', 'are', \"should've\", 'over', 'haven', 'll', 'yours', 'his', \"mightn't\", \"couldn't\", 'ma', 'hasn', 'should', 'but', 'up', 'he', 'by', 'being', 'isn', 'shan', 'what', 'why', 'an', 'these', 'or', 'which', 'mightn', 'for', 'am', \"wouldn't\", 'were', 'won', 'itself', 'their', 'after', 'do', \"aren't\", 'own', 'few', 'some', 'couldn', 'on', 'hers', 'didn', 'weren', 'than', 'ourselves', 'whom', 'there', 'only', 'our', 'needn', 'because', 'as', 'too', 'to', 'who', 'not', 'then', 'until', 'this', 'through', 'down', 'once', \"she's\", 'y', 'same', 'such', \"isn't\", 'while', 'against', 'themselves', 'both'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-14-92626ad99c5f>:62: UserWarning: WhiteSpacePreprocessing is deprecated and will be removed in future versions.Use WhiteSpacePreprocessingStopwords.\n",
            "  warnings.warn(\"WhiteSpacePreprocessing is deprecated and will be removed in future versions.\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['interested career cybersecurity join us wednesday october 19 1200 pm 200 pm et career panel three esteemed leaders industry spots limited reserve today main benefits web3 data security cybersecurity bigdata privacy python javascript cloud technology devcommunity coding developers software aws serverless webdevelopment opensource iot cybersecurity wheel raffle prize bugs bags amp brews event win bags amp boards saturday schram brewery chaska plus become ally fight endthestigma proceeds go mental health initiatives public schools 16 pm see secured 125m bear season roll mainnet building next big thing sma contracts iot datasecurity infosec 100daysofcode hacking cybersecurity ai crypto cryptonews smacities smahome idor checklist bugbounty bugbountytips cybersecurity']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}